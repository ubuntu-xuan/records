http://cwiki.apachecn.org/display/Spark/Spark+Streaming

Spark Streaming
1.初始化 StreamingContext，它是所有的 Spark Streaming 功能的主入口点
  sc = SparkContext(appName="PythonStreamingKafkaWordCount")
  ssc = StreamingContext(sc, 1)

一个 context 定义之后，你必须做以下几个方面。
    通过创建输入 DStreams 定义输入源。
    通过应用转换和输出操作 DStreams 定义流计算（streaming computations）。
    开始接收数据，并用 streamingContext.start() 处理它。
    等待处理被停止（手动停止或者因为任何错误停止）使用 StreamingContext.awaitTermination() 。
    该处理可以使用 streamingContext.stop() 手动停止。

要记住的要点 : 
    一旦一个 context 已经启动，将不会有新的数据流的计算可以被创建或者添加到它。
    一旦一个 context 已经停止，它不会被重新启动。
    同一时间内在 JVM 中只有一个 StreamingContext 可以被激活。
    在 StreamingContext 上的 stop() 同样也停止了 SparkContext 。为了只停止 StreamingContext ，设置 stop() 的可选参数，名叫 stopSparkContext 为 false 。
    一个 SparkContext 就可以被重用以创建多个 StreamingContexts，只要前一个 StreamingContext 在下一个StreamingContext 被创建之前停止（不停止 SparkContext）。 


2.Discretized Stream（离散化流）或者 DStream（离散流）
  是 Spark Streaming 提供的基本抽象。它代表了一个连续的数据流，无论是从源接收到的输入数据流，还是通过变换输入流所产生的处理过的数据流。在内部，一个离散流（DStream）被表示为一系列连续的 RDDs，RDD 是 Spark 的一个不可改变的，分布式的数据集的抽象

3.Input DStreams 和 Receivers
  Spark Streaming 提供了两种内置的流来源（streaming source）。
    .Basic sources（基本来源）: 在 StreamingContext API 中直接可用的源（source）。例如，文件系统（file systems），和 socket 连接（socket connections）。
    .Advanced sources（高级来源）: 就像 Kafka，Flume，Kinesis 之类的通过额外的实体类来使用的来源。这些都需要连接额外的依赖，就像在 连接 部分的讨论。

  基本来源：
  1.ssc.socketTextStream(),通过一个TCP socket连接接收到的文本数据中创建一个离散流（DStream）
  2.根据文件作为输入来源
    .文件流(File Streams):用于从文件中读取数据，在任何与HDFS API兼容的文件系统中（HDFS,S3,NFS）
	Spark Streaming将监控dataDirectory目录，并处理任何在该目录下创建的文件（写在嵌套目录中的文件是不支持的）。注意 : 
	        文件必须具有相同的数据格式。
    		文件必须在 dataDirectory 目录中通过原子移动或者重命名它们到这个 dataDirectory 目录下来创建。
    		一旦移动，这些文件必须不能再更改，因此如果文件被连续地追加，新的数据将不会被读取。
	
    对于简单的文本文件，还有一个更加简单的方法 streamingContext.textFileStream(dataDirectory)。并且文件流（file streams）不需要运行一个接收器（receiver），因此，不需要分配内核（core）。
    在 Python API 中 Python API fileStream 是不可用的，只有 textFileStream 是可用的。
  3.Streams based on Custom Receivers（基于自定义的接收器的流）: 离散流（DStreams）可以使用通过自定义的接收器接收到的数据来创建。查看 自定义接收器指南 来了解更多细节。
  4.Queue of RDDs as a Stream（RDDs 队列作为一个流）: 为了使用测试数据测试 Spark Streaming 应用程序，还可以使用 streamingContext.queueStream(queueOfRDDs) 创建一个基于 RDDs 队列的离散流（DStream），每个进入队列的 RDD 都将被视为 DStream 中的一个批次数据，并且就像一个流进行处理。


  Advanced Sources（高级来源）
    Python API  在 Spark 2.0.2 中，这些来源中，Kafka，Kinesis 和 Flume 在 Python API 中都是可用的。
    这一类别的来源需要使用非 Spark 库中的外部接口，它们中的其中一些还需要比较复杂的依赖关系（例如， Kafka 和 Flume）。因此，为了最小化有关的依赖关系的版本冲突的问题，这些资源本身不能创建 DStream 的功能，它是通过连接单独的类库实现创建 DStream 的功能。
    需要注意的是这些高级来源在 Spark Shell 中是不可用的。因此，基于这些高级来源的应用程序不能在 shell 中被测试。如果你真的想要在 Spark shell 中使用它们，你必须下载带有它的依赖的相应的 Maven 组件的 JAR ，并且将其添加到 classpath 。
    一些高级来源如下。
      Kafka : Spark Streaming 2.0.2 与 Kafka 0.8.2.1 以及更高版本兼容。查看 Kafka 集成指南 来了解更多细节。
      Flume : Spark Streaming 2.0.2 与 Flume 1.6.0 兼容。查看 Flume 集成指南 来了解更多细节。
      Kinesis : Spark Streaming 2.0.2 与 Kinesis 客户端库 1.2.1 兼容。查看 Kinesis 集成指南 来了解更多细节。

  Custom Sources（自定义来源）
    在 Python 中 Python API 还不支持自定义来源。
    输入离散流（Input DStreams）也可以从创建自定义数据源。所有你需要做的就是实现一个用户定义（user-defined）的接收器（receiver）（查看下一章节去了解那是什么），这个接收器可以从自定义的数据源接收数据并将它推送到 Spark 。

  Reveiver Reliability（接收器的可靠性）
    可以有两种基于他们的可靠性的数据源。数据源（如 Kafka 和 Flume）允许传输的数据被确认。如果系统从这些可靠的数据来源接收数据，并且被确认（acknowledges）正确地接收数据，它可以确保数据不会因为任何类型的失败而导致数据丢失。这样就出现了 2 种接收器（receivers）: 
      Reliable Receiver（可靠的接收器）- 当数据被接收并存储在 Spark 中并带有备份副本时，一个可靠的接收器（reliable receiver）正确地发送确认（acknowledgment）给一个可靠的数据源（reliable source）。
      Unreliable Receiver（不可靠的接收器）- 一个不可靠的接收器（ unreliable receiver ）不发送确认（acknowledgment）到数据源。这可以用于不支持确认的数据源，或者甚至是可靠的数据源当你不想或者不需要进行复杂的确认的时候。
  在 自定义接收器指南（Custom Receiver Guide） 中描述了关于如何去编写一个可靠的接收器的细节。


DStreams 上的 Transformations（转换）
  与 RDD 类似，transformation 允许从 input DStream 输入的数据做修改。DStreams 支持很多在 RDD 中可用的 transformation 算子。一些常用的算子如下所示 : 
  map(func)	利用函数 func 处理原 DStream 的每个元素，返回一个新的 DStream。
  flatMap(func)	与 map 相似，但是每个输入项可用被映射为 0 个或者多个输出项。
  filter(func)	返回一个新的 DStream，它仅仅包含源 DStream 中满足函数 func 的项。
  repartition(numPartitions)	通过创建更多或者更少的 partition 改变这个 DStream 的并行级别（level of parallelism）。
  union(otherStream)	返回一个新的 DStream，它包含源 DStream 和 otherStream 的所有元素。
  count()	通过计算源 DStream 中每个 RDD 的元素数量，返回一个包含单元素（single-element）RDDs 的新 DStream。
  reduce(func)	利用函数 func 聚集源 DStream 中每个 RDD 的元素，返回一个包含单元素（single-element）RDDs 的新 DStream。函数应该是相关联的，以使计算可以并行化。
  countByValue()	这个算子应用于元素类型为 K 的 DStream上，返回一个（K,long）对的新 DStream，每个键的值是在原 DStream 的每个 RDD 中的频率。
  reduceByKey(func, [numTasks])	
    当在一个由 (K,V) 对组成的 DStream 上调用这个算子，返回一个新的由 (K,V) 对组成的 DStream，每一个 key 的值均由给定的 reduce 函数聚集起来。
    注意：在默认情况下，这个算子利用了 Spark 默认的并发任务数去分组。你可以用 numTasks 参数设置不同的任务数。
  join(otherStream, [numTasks])	当应用于两个 DStream（一个包含（K,V）对，一个包含 (K,W) 对），返回一个包含 (K, (V, W)) 对的新 DStream。
  cogroup(otherStream, [numTasks])	当应用于两个 DStream（一个包含（K,V）对，一个包含 (K,W) 对），返回一个包含 (K, Seq[V], Seq[W]) 的元组。
  transform(func)	通过对源 DStream 的每个 RDD 应用 RDD-to-RDD 函数，创建一个新的 DStream。这个可以在 DStream 中的任何 RDD 操作中使用。
  updateStateByKey(func)	利用给定的函数更新 DStream 的状态，返回一个新 "state" 的 DStream。

  Transform 操作
    transform 操作（以及它的变化形式如 transformWith）允许在 DStream 运行任何 RDD-to-RDD 函数。它能够被用来应用任何没在 DStream API 中提供的 RDD 操作（It can be used to apply any RDD operation that is not exposed in the DStream API）。 例如，连接数据流中的每个批（batch）和另外一个数据集的功能并没有在 DStream API 中提供，然而你可以简单的利用 transform 方法做到

* Window （窗口）操作
  如上图显示，窗口在源 DStream 上滑动，合并和操作落入窗内的源 RDDs，产生窗口化的 DStream 的 RDDs。在这个具体的例子中，程序在三个时间单元的数据上进行窗口操作，并且每两个时间单元滑动一次。 这说明，任何一个窗口操作都需要指定两个参数 : 
    window length（窗口长度）: 窗口的持续时间。
    sliding interval（滑动的时间间隔）: 窗口操作执行的时间间隔。
这两个参数必须是源 DStream 上  batch interval（批时间间隔）的倍数

一些常用的窗口操作如下所示，这些操作都需要用到上文提到的两个参数 - 窗口长度和滑动的时间间隔。
  window ( windowLength , slideInterval )	返回一个新的 DStream，基于窗口的批 DStream 来源。
  countByWindow ( windowLength , slideInterval )	返回一个滑动窗口中的元素计算流。
  reduceByWindow ( func, windowLength , slideInterval )	返回一个新创建的单个元素流,通过聚合元素流了 滑动时间间隔使用 函数 。 函数应该关联和交换,以便它可以计算 正确地并行执行。
  reduceByKeyAndWindow ( func, windowLength , slideInterval , ( numTasks ])	当呼吁DStream(K、V)对,返回一个新的DStream(K、V) 对每个键的值在哪里聚合使用给定的reduce函数 函数 在一个滑动窗口批次。 注意: 默认情况下,它使用引发的默认数量 并行任务(2为本地模式,在集群模式是由配置数量 财产 spark.default.parallelism 分组)。 你可以通过一个可选的 numTasks 参数设置不同数量的任务。
  reduceByKeyAndWindow ( func, invFunc , windowLength , slideInterval ,( numTasks ])	
	上面的reduceByKeyAndWindow()的一个更有效的版本，其中每个窗口的reduce值是使用上一个窗口的reduce值递增计算的。 这是通过减少进入滑动窗口的新数据和“反向减少”离开窗口的旧数据来完成的。 一个例子是在窗口滑动时“添加”和“减去”键的计数。 然而，它仅适用于“可逆缩减函数”，即，具有对应的“逆缩减”函数（作为参数invFunc）的那些缩减函数。 像reduceByKeyAndWindow中一样，reduce任务的数量可通过可选参数进行配置。 请注意，必须启用检查点设置才能使用此操作。
countByValueAndWindow ( windowLength , slideInterval ,[ numTasks ])	当呼吁DStream(K、V)对,返回一个新的DStream(K,长)对的 每个键的值是它的频率在一个滑动窗口。 就像在 reduceByKeyAndWindow ,通过一个减少任务的数量是可配置的 可选参数。

* Join 操作
最后，Spark streaming 可以很容易与其它的数据源进行 join。

*Stream-stream 连接
  stream 可以很容易与其他 stream 进行 join: stream1.join(stream2)
  之类，在每一个批间隔中，生成的抽样 stream1 将与生成的抽样 stream2 进行 join 操作。 也可以做 leftOuterJoin ，rightOuterJoin，fullOuterJoin。此外，它通常是非常有用的做连接的窗口 (window) stream。 这是非常容易的  

* Stream-dataset连接
 这已经被证明在早些时候解释  DStream.transform 操作。


DStreams上的输出操作
  输出操作允许将 DStream 的数据推送到外部系统, 如数据库或文件系统. 由于输出操作实际上允许外部系统使用变换后的数据, 所以它们触发所有 DStream 变换的实际执行（类似于RDD的动作）. 目前, 定义了以下输出操作：

.print() 	在运行流应用程序的 driver 节点上的DStream中打印每批数据的前十个元素. 这对于开发和调试很有用.
		Python API 这在 Python API 中称为 pprint(). 	
.saveAsTextFiles(prefix, [suffix]) 	将此 DStream 的内容另存为文本文件. 每个批处理间隔的文件名是根据 前缀 和 后缀 : "prefix-TIMEIN_MS[.suffix]"_ 生成的.
.saveAsObjectFiles(prefix, [suffix]) 	将此 DStream 的内容另存为序列化 Java 对象的 SequenceFiles. 每个批处理间隔的文件名是根据 前缀 和 后缀 : "prefix-TIMEIN_MS[.suffix]"_ 生成的.
Python API 这在Python API中是不可用的. 	
.saveAsHadoopFiles(prefix, [suffix]) 	将此 DStream 的内容另存为 Hadoop 文件. 每个批处理间隔的文件名是根据 前缀 和 后缀 : "prefix-TIMEIN_MS[.suffix]"_ 生成的.
					Python API 这在Python API中是不可用的. 	
.foreachRDD(func) 	对从流中生成的每个 RDD 应用函数 func 的最通用的输出运算符. 此功能应将每个 RDD 中的数据推送到外部系统, 例如将 RDD 保存到文件, 或将其通过网络写入数据库. 请注意, 函数 func 在运行流应用程序的 driver 进程中执行, 通常会在其中具有 RDD 动作, 这将强制流式传输 RDD 的计算.


foreachRDD 设计模式的使用
  dstream.foreachRDD 是一个强大的原语, 允许将数据发送到外部系统.但是, 了解如何正确有效地使用这个原语很重要. 避免一些常见的错误如下.
  通常向外部系统写入数据需要创建连接对象（例如与远程服务器的 TCP 连接）, 并使用它将数据发送到远程系统.为此, 开发人员可能会无意中尝试在Spark driver 中创建连接对象, 然后尝试在Spark工作人员中使用它来在RDD中保存记录  
  
  def sendRecord(rdd):
      connection = createNewConnection()  # executed at the driver
      rdd.foreach(lambda record: connection.send(record))
      connection.close()

  dstream.foreachRDD(sendRecord)
  这是不正确的, 因为这需要将连接对象序列化并从 driver 发送到 worker. 这种连接对象很少能跨机器转移. 此错误可能会显示为序列化错误（连接对象不可序列化）, 初始化错误（连接对象需要在 worker 初始化）等. 正确的解决方案是在 worker 创建连接对象.
  但是, 这可能会导致另一个常见的错误 - 为每个记录创建一个新的连接. 例如:
    def sendRecord(record):
        connection = createNewConnection()
        connection.send(record)
        connection.close()

    dstream.foreachRDD(lambda rdd: rdd.foreach(sendRecord))

  通常, 创建连接对象具有时间和资源开销. 因此, 创建和销毁每个记录的连接对象可能会引起不必要的高开销, 并可显着降低系统的总体吞吐量. 一个更好的解决方案是使用 rdd.foreachPartition - 创建一个连接对象, 并使用该连接在 RDD 分区中发送所有记录.

  def sendPartition(iter):
      connection = createNewConnection()
      for record in iter:
          connection.send(record)
      connection.close()

  dstream.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))

这样可以在多个记录上分摊连接创建开销.

最后, 可以通过跨多个RDD /批次重用连接对象来进一步优化. 可以维护连接对象的静态池, 而不是将多个批次的 RDD 推送到外部系统时重新使用, 从而进一步减少开销.

def sendPartition(iter):
    # ConnectionPool is a static, lazily initialized pool of connections
    connection = ConnectionPool.getConnection()
    for record in iter:
        connection.send(record)
    # return to the pool for future reuse
    ConnectionPool.returnConnection(connection)

dstream.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))

请注意, 池中的连接应根据需要懒惰创建, 如果不使用一段时间, 则会超时. 这实现了最有效地将数据发送到外部系统.

DataFrame 和 SQL 操作
在流数据上使用 DataFrames and SQL 和 SQL 操作
https://github.com/apache/spark/blob/v2.2.0/examples/src/main/python/streaming/sql_network_wordcount.py

#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

"""
 Use DataFrames and SQL to count words in UTF8 encoded, '\n' delimited text received from the
 network every second.
 Usage: sql_network_wordcount.py <hostname> <port>
   <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
 To run this on your local machine, you need to first run a Netcat server
    `$ nc -lk 9999`
 and then run the example
    `$ bin/spark-submit examples/src/main/python/streaming/sql_network_wordcount.py localhost 9999`
"""
from __future__ import print_function

import sys

from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.sql import Row, SparkSession


def getSparkSessionInstance(sparkConf):
    if ('sparkSessionSingletonInstance' not in globals()):
        globals()['sparkSessionSingletonInstance'] = SparkSession\
            .builder\
            .config(conf=sparkConf)\
            .getOrCreate()
    return globals()['sparkSessionSingletonInstance']


if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: sql_network_wordcount.py <hostname> <port> ", file=sys.stderr)
        exit(-1)
    host, port = sys.argv[1:]
    sc = SparkContext(appName="PythonSqlNetworkWordCount")
    ssc = StreamingContext(sc, 1)

    # Create a socket stream on target ip:port and count the
    # words in input stream of \n delimited text (eg. generated by 'nc')
    lines = ssc.socketTextStream(host, int(port))
    words = lines.flatMap(lambda line: line.split(" "))

    # Convert RDDs of the words DStream to DataFrame and run SQL query
    def process(time, rdd):
        print("========= %s =========" % str(time))

        try:
            # Get the singleton instance of SparkSession
            spark = getSparkSessionInstance(rdd.context.getConf())

            # Convert RDD[String] to RDD[Row] to DataFrame
            rowRdd = rdd.map(lambda w: Row(word=w))
            wordsDataFrame = spark.createDataFrame(rowRdd)

            # Creates a temporary view using the DataFrame.
            wordsDataFrame.createOrReplaceTempView("words")

            # Do word count on table using SQL and print it
            wordCountsDataFrame = \
                spark.sql("select word, count(*) as total from words group by word")
            wordCountsDataFrame.show()
        except:
            pass

    words.foreachRDD(process)
    ssc.start()
ssc.awaitTermination()









