https://blog.csdn.net/Koala_Tree/article/details/78725881


Seaborn绘图：
https://blog.csdn.net/u013082989/article/details/73278458


1. 存活的比例 -- 饼形图
train_data['Survived'].value_counts().plot.pie(autopct = '%1.2f%%')

2.缺失值处理
（1）如果数据集很多，但有很少的缺失值，可以删掉带缺失值的行；
（2）如果该属性相对学习来说不是很重要，可以对缺失值赋均值或者众数。比如在哪儿上船Embarked这一属性（共有三个上船地点），缺失俩值，可以用众数赋值train_data.Embarked[train_data.Embarked.isnull()] = train_data.Embarked.dropna().mode().values
3）对于标称属性，可以赋一个代表缺失的值，比如‘U0’。因为缺失本身也可能代表着一些隐含信息。比如船舱号Cabin这一属性，缺失可能代表并没有船舱。
#replace missing value with U0
train_data['Cabin'] = train_data.Cabin.fillna('U0') # train_data.Cabin[train_data.Cabin.isnull()]='U0'
（4）使用回归 随机森林等模型来预测缺失属性的值。因为Age在该数据集里是一个相当重要的特征（先对Age进行分析即可得知），所以保证一定的缺失值填充准确率是非常重要的，对结果也会产生较大影响。一般情况下，会使用数据完整的条目作为模型的训练集，以此来预测缺失值。对于当前的这个数据，可以使用随机森林来预测也可以使用线性回归预测。这里使用随机森林预测模型，选取数据集中的数值属性作为特征（因为sklearn的模型只能处理数值属性，所以这里先仅选取数值特征，但在实际的应用中需要将非数值特征转换为数值特征）

3.分析数据之间的关系
(1) 性别与是否生存的关系 Sex -- .plot.bar() 条形图
  train_data.groupby(['Sex','Survived'])['Survived'].count()
  train_data[['Sex','Survived']].groupby(['Sex']).mean().plot.bar()
(2) 船舱等级和生存与否的关系 Pclass
(3) 年龄与存活与否的关系 Age  
    --  小提琴图 (Violin Plot) 用于显示数据分布及其概率密度  这种图表结合了箱形图和密度图的特征
    https://blog.csdn.net/ac540101928/article/details/79235591
    总体的年龄分布 -- hist  直方图 
    -- boxplot 箱形图
    不同年龄下的生存和非生存的分布情况 -- FacetGrid
    不同年龄下的平均生存率 -- barplot
    划分数据 -- pd.cut(train_data['Age'], bins)

(4) 称呼与存活与否的关系 Name
      train_data['Title'] = train_data['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
      pd.crosstab(train_data['Title'], train_data['Sex'])
      观察不同称呼与生存率的关系：
	train_data[['Title','Survived']].groupby(['Title']).mean().plot.bar()
(5) 有无兄弟姐妹和存活与否的关系 SibSp
(6) 有无父母子女和存活与否的关系 Parch
(7) 亲友的人数和存活与否的关系 SibSp & Parch
(8) 票价分布和存活与否的关系 Fare
(9) 船舱类型和存活与否的关系 Cabin
由于船舱的缺失值确实太多，有效值仅仅有204个，很难分析出不同的船舱和存活的关系，所以在做特征工程的时候，可以直接将该组特征丢弃。

当然，这里我们也可以对其进行一下分析，对于缺失的数据都分为一类。

简单地将数据分为是否有Cabin记录作为特征，与生存与否进行分析：

# Replace missing values with "U0"
train_data.loc[train_data.Cabin.isnull(), 'Cabin'] = 'U0'
train_data['Has_Cabin'] = train_data['Cabin'].apply(lambda x: 0 if x == 'U0' else 1)
train_data[['Has_Cabin','Survived']].groupby(['Has_Cabin']).mean().plot.bar()
对不同类型的船舱进行分析：

# create feature for the alphabetical part of the cabin number
train_data['CabinLetter'] = train_data['Cabin'].map(lambda x: re.compile("([a-zA-Z]+)").search(x).group())
# convert the distinct cabin letters with incremental integer values
train_data['CabinLetter'] = pd.factorize(train_data['CabinLetter'])[0]
train_data[['CabinLetter','Survived']].groupby(['CabinLetter']).mean().plot.bar()
可见，不同的船舱生存率也有不同，但是差别不大。所以在处理中，我们可以直接将特征删除。
(10) 港口和存活与否的关系 Embarked  -- countplot
    sns.countplot('Embarked', hue='Survived', data=train_data)
    plt.title('Embarked and Survived')

    -- factorplot ：可以通过这个函数绘制以上几种图，指定kind即可，有point, bar, count, box, violin, strip
    		    row和col指定绘制的行数和列数，给出一个种类类型的列名即可

    sns.factorplot('Embarked', 'Survived', data=train_data, size=3, aspect=2)
    plt.title('Embarked and Survived rate')
    plt.show()

(11) 其他可能和存活与否有关系的特征
    对于数据集中没有给出的特征信息，我们还可以联想其他可能会对模型产生影响的特征因素。如：乘客的国籍、乘客的身高、乘客的体重、乘客是否会游泳、乘客职业等等。
另外还有数据集中没有分析的几个特征：Ticket（船票号）、Cabin（船舱号）,这些因素的不同可能会影响乘客在船中的位置从而影响逃生的顺序。但是船舱号数据缺失，船票号类别大，难以分析规律，所以在后期模型融合的时候，将这些因素交由模型来决定其重要性。


4. 变量转换
变量转换的目的是将数据转换为适用于模型使用的数据，不同模型接受不同类型的数据，Scikit-learn要求数据都是数字型numeric，所以我们要将一些非数字型的原始数据转换为数字型numeric。

所以下面对数据的转换进行介绍，以在进行特征工程的时候使用。

所有的数据可以分为两类：
    1.定性(Quantitative)变量可以以某种方式排序，Age就是一个很好的列子。
    2.定量(Qualitative)变量描述了物体的某一（不能被数学表示的）方面，Embarked就是一个例子。

定性(Qualitative)转换：
1. Dummy Variables
    就是类别变量或者二元变量，当qualitative variable是一些频繁出现的几个独立变量时，Dummy Variables比较适合使用。我们以Embarked为例，Embarked只包含三个值’S’,’C’,’Q’，我们可以使用下面的代码将其转换为dummies:
    embark_dummies  = pd.get_dummies(train_data['Embarked'])
    train_data = train_data.join(embark_dummies)
    train_data.drop(['Embarked'], axis=1,inplace=True)

2. Factorizing(one-hot)
    dummy不好处理Cabin（船舱号）这种标称属性，因为他出现的变量比较多。所以Pandas有一个方法叫做factorize()，它可以创建一些数字，来表示类别变量，对每一个类别映射一个ID，这种映射最后只生成一个特征，不像dummy那样生成多个特征。

# Replace missing values with "U0"
train_data['Cabin'][train_data.Cabin.isnull()] = 'U0'
# create feature for the alphabetical part of the cabin number
train_data['CabinLetter'] = train_data['Cabin'].map( lambda x : re.compile("([a-zA-Z]+)").search(x).group())
# convert the distinct cabin letters with incremental integer values
train_data['CabinLetter'] = pd.factorize(train_data['CabinLetter'])[0]

train_data['CabinLetter'].head()

定量(Quantitative)转换：
1. Scaling
	Scaling可以将一个很大范围的数值映射到一个很小的范围(通常是-1 - 1，或则是0 - 1)，很多情况下我们需要将数值做Scaling使其范围大小一样，否则大范围数值特征将会由更高的权重。比如：Age的范围可能只是0-100，而income的范围可能是0-10000000，在某些对数组大小敏感的模型中会影响其结果。
	scaler = preprocessing.StandardScaler()
	train_data['Age_scaled'] = scaler.fit_transform(train_data['Age'].values.reshape(-1, 1))

2. Binning
	Binning通过观察“邻居”(即周围的值)将连续数据离散化。存储的值被分布到一些“桶”或“箱“”中，就像直方图的bin将数据划分成几块一样。
	在将数据Bining化后，要么将数据factorize化，要么dummies化。
5. 特征工程

在进行特征工程的时候，我们不仅需要对训练数据进行处理，还需要同时将测试数据同训练数据一起处理，使得二者具有相同的数据类型和数据分布。

train_df_org = pd.read_csv('data/train.csv')
test_df_org = pd.read_csv('data/test.csv')
test_df_org['Survived'] = 0
combined_train_test = train_df_org.append(test_df_org)
PassengerId = test_df_org['PassengerId']


对数据进行特征工程，也就是从各项参数中提取出对输出结果有或大或小的影响的特征，将这些特征作为训练模型的依据。 一般来说，我们会先从含有缺失值的特征开始。

1.Embarked 以众数来填充
2.sex one-hot
3.Name 从名字中提取各种称呼：

# what is each person's title? 
combined_train_test['Title'] = combined_train_test['Name'].map(lambda x: re.compile(", (.*?)\.").findall(x)[0])

将各式称呼进行统一化处理：

title_Dict = {}
title_Dict.update(dict.fromkeys(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer'))
title_Dict.update(dict.fromkeys(['Don', 'Sir', 'the Countess', 'Dona', 'Lady'], 'Royalty'))
title_Dict.update(dict.fromkeys(['Mme', 'Ms', 'Mrs'], 'Mrs'))
title_Dict.update(dict.fromkeys(['Mlle', 'Miss'], 'Miss'))
title_Dict.update(dict.fromkeys(['Mr'], 'Mr'))
title_Dict.update(dict.fromkeys(['Master','Jonkheer'], 'Master'))
4.Fare 按照各自的均值来填充
5.Pclass 分出每等舱里的高价和低价位
6.Parch and SibSp 亲友的数量没有或者太多会影响到Survived。所以将二者合并为FamliySize这一组合项
7.Age 常见的有两种对年龄的填充方式：一种是根据Title中的称呼，如Mr，Master、Miss等称呼不同类别的人的平均年龄来填充；一种是综合几项如Sex、Title、Pclass等其他没有缺失值的项，使用机器学习算法来预测Age。
   建立Age的预测模型，我们可以多模型预测，然后再做模型的融合，提高预测的精度
8.Ticket 将Ticket中的字母分开，为数字的部分则分为一类
9.Cabin Cabin项的缺失值确实太多了，我们很难对其进行分析，或者预测。所以这里我们可以直接将Cabin这一项特征去除。但通过上面的分析，可以知道，该特征信息的有无也与生存率有一定的关系，所以这里我们暂时保留该特征，并将其分为有和无两类


特征间相关性分析
挑选一些主要的特征，生成特征之间的关联图，查看特征与特征之间的相关性：
  -- heatmap 热力图


Correlation = pd.DataFrame(combined_train_test[
    ['Embarked', 'Sex', 'Title', 'Name_length', 'Family_Size', 'Family_Size_Category','Fare', 'Fare_bin_id', 'Pclass', 
     'Pclass_Fare_Category', 'Age', 'Ticket_Letter', 'Cabin']])
挑选一些主要的特征，生成特征之间的关联图，查看特征与特征之间的相关性：

Correlation = pd.DataFrame(combined_train_test[
    ['Embarked', 'Sex', 'Title', 'Name_length', 'Family_Size', 'Family_Size_Category','Fare', 'Fare_bin_id', 'Pclass', 
     'Pclass_Fare_Category', 'Age', 'Ticket_Letter', 'Cabin']])

colormap = plt.cm.viridis plt.figure(figsize=(14,12)) plt.title('Pearson Correlation of Features', y=1.05, size=15) sns.heatmap(Correlation.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)

特征之间的数据分布图
g = sns.pairplot(combined_train_test[[u'Survived', u'Pclass', u'Sex', u'Age', u'Fare', u'Embarked', u'Family_Size', u'Title', u'Ticket_Letter']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) ) g.set(xticklabels=[])

输入模型前的一些处理：
1. 一些数据的正则化
preprocessing.StandardScaler().fit

2. 弃掉无用特征
3. 将训练数据和测试数据分开

6. 模型融合及测试

(3) 模型融合（Model Ensemble）
  常见的模型融合方法有：Bagging、Boosting、Stacking、Blending。

Model Ensemble
  常见的模型融合方法有：Bagging、Boosting、Stacking、Blending。
  
  Bagging
    1.Bagging 将多个模型，也就是多个基学习器的预测结果进行简单的加权平均或者投票。它的好处是可以并行地训练基学习器。Random Forest就用到了Bagging的思想.
    2.Boosting
    Boosting 的思想有点像知错能改，每个基学习器是在上一个基学习器学习的基础上，对上一个基学习器的错误进行弥补。我们将会用到的 AdaBoost，Gradient Boost 就用到了这种思想。
    3.Stacking
    Stacking是用新的次学习器去学习如何组合上一层的基学习器。如果把 Bagging 看作是多个基分类器的线性组合，那么Stacking就是多个基分类器的非线性组合。Stacking可以将学习器一层一层地堆砌起来，形成一个网状的结构。
    相比来说Stacking的融合框架相对前面的二者来说在精度上确实有一定的提升，所以在下面的模型融合上，我们也使用Stacking方法。
    4.Blending
    Blending 和 Stacking 很相似，但同时它可以防止信息泄露的问题。

(1) 利用不同的模型来对特征进行筛选，选出较为重要的特征：
 
def get_top_n_features(titanic_train_data_X, titanic_train_data_Y, top_n_features):
    # random forest
    rf_est = RandomForestClassifier(random_state=0)
 
    # AdaBoost
    ada_est =AdaBoostClassifier(random_state=0)

    # ExtraTree
    et_est = ExtraTreesClassifier(random_state=0)

    # GradientBoosting
    gb_est =GradientBoostingClassifier(random_state=0)

    # DecisionTree
    dt_est = DecisionTreeClassifier(random_state=0)

    # merge the three models 
    features_top_n = pd.concat([features_top_n_rf, features_top_n_ada, features_top_n_et, features_top_n_gb,       
    features_top_n_dt], ignore_index=True).drop_duplicates() 
    features_importance = pd.concat([feature_imp_sorted_rf, feature_imp_sorted_ada, feature_imp_sorted_et, feature_imp_sorted_gb,    
                                    feature_imp_sorted_dt],ignore_index=True) 
    return features_top_n , features_importance
 

(2) 依据我们筛选出的特征构建训练集和测试集
	但如果在进行特征工程的过程中，产生了大量的特征，而特征与特征之间会存在一定的相关性。太多的特征一方面会影响模型训练的速度，另一方面也可能会使得模型过拟合。所以在特征太多的情况下，我们可以利用不同的模型对特征进行筛选，选取出我们想要的前n个特征。
	feature_to_pick = 30
	feature_top_n, feature_importance = get_top_n_features(titanic_train_data_X, titanic_train_data_Y, feature_to_pick)
	titanic_train_data_X = pd.DataFrame(titanic_train_data_X[feature_top_n])
	titanic_test_data_X = pd.DataFrame(titanic_test_data_X[feature_top_n])

用视图可视化不同算法筛选的特征排序：
plt.barh


Stacking框架融合:

这里我们使用了两层的模型融合，Level 1使用了：RandomForest、AdaBoost、ExtraTrees、GBDT、DecisionTree、KNN、SVM ，一共7个模型，Level 2使用了XGBoost使用第一层预测的结果作为特征对最终的结果进行预测。
Level 1：

Stacking框架是堆叠使用基础分类器的预测作为对二级模型的训练的输入。 然而，我们不能简单地在全部训练数据上训练基本模型，产生预测，输出用于第二层的训练。如果我们在Train Data上训练，然后在Train Data上预测，就会造成标签。为了避免标签，我们需要对每个基学习器使用K-fold，将K个模型对Valid Set的预测结果拼起来，作为下一层学习器的输入。

所以这里我们建立输出fold预测方法：


from sklearn.model_selection import KFold # Some useful parameters which will come in handy later on 

ntrain = titanic_train_data_X.shape[0]  # titanic_train_data_X取出30维特征的训练集
ntest = titanic_test_data_X.shape[0] 
SEED = 0 # for reproducibility 
NFOLDS = 7 # set folds for out-of-fold prediction 
# k折
kf = KFold(n_splits = NFOLDS, random_state=SEED, shuffle=False)

def get_out_fold(clf, x_train, y_train, x_test): 
    oof_train = np.zeros((ntrain,)) 
    oof_test = np.zeros((ntest,)) 
    oof_test_skf = np.empty((NFOLDS, ntest)) 
    for i, (train_index, test_index) in enumerate(kf.split(x_train)): 
        # 对第i折
        x_tr = x_train[train_index] 
        y_tr = y_train[train_index]
				# test_index：分成测试集
        x_te = x_train[test_index] 
				# 用第i折分出的训练集训练模型
        clf.fit(x_tr, y_tr) 
				# 用第i折分出的测试集预测结果
        oof_train[test_index] = clf.predict(x_te)
				# 传入测试集，预测，保存第i折的结果
        oof_test_skf[i, :] = clf.predict(x_test) 

    # 最终结果返回的是经过k折后的平均值
    oof_test[:] = oof_test_skf.mean(axis=0) 

    # oof_train的预测结果， 传入的测试集的预测结果
    # reshape(-1, 1) 变成1列
    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)




构建不同的基学习器，这里我们使用了RandomForest、AdaBoost、ExtraTrees、GBDT、DecisionTree、KNN、SVM 七个基学习器：（这里的模型可以使用如上面的GridSearch方法对模型的超参数进行搜索选择）：

from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC


rf = RandomForestClassifier(n_estimators=500, warm_start=True, max_features='sqrt',max_depth=6, min_samples_split=3, min_samples_leaf=2, n_jobs=-1, verbose=0)
ada = AdaBoostClassifier(n_estimators=500, learning_rate=0.1)
et = ExtraTreesClassifier(n_estimators=500, n_jobs=-1, max_depth=8, min_samples_leaf=2, verbose=0) 
gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.008, min_samples_split=3, min_samples_leaf=2, max_depth=5, verbose=0) 
dt = DecisionTreeClassifier(max_depth=8) knn = KNeighborsClassifier(n_neighbors = 2) 
svm = SVC(kernel='linear', C=0.025)


将pandas转换为arrays：
# Create Numpy arrays of train, test and target (Survived) dataframes to feed into our models 
x_train = titanic_train_data_X.values # Creates an array of the train data 
x_test = titanic_test_data_X.values # Creats an array of the test data 
y_train = titanic_train_data_Y.values


# Create our OOF train and test predictions. These base results will be used as new features 
# 返回训练集和测试集
rf_oof_train, rf_oof_test = get_out_fold(rf, x_train, y_train, x_test) # Random Forest 
ada_oof_train, ada_oof_test = get_out_fold(ada, x_train, y_train, x_test) # AdaBoost 
et_oof_train, et_oof_test = get_out_fold(et, x_train, y_train, x_test) # Extra Trees 
gb_oof_train, gb_oof_test = get_out_fold(gb, x_train, y_train, x_test) # Gradient Boost 
dt_oof_train, dt_oof_test = get_out_fold(dt, x_train, y_train, x_test) # Decision Tree 
knn_oof_train, knn_oof_test = get_out_fold(knn, x_train, y_train, x_test) # KNeighbors 
svm_oof_train, svm_oof_test = get_out_fold(svm, x_train, y_train, x_test) # Support Vector 
print("Training is complete")



(4) 预测并生成提交文件
Level 2：

我们利用XGBoost，使用第一层预测的结果作为特征对最终的结果进行预测。

# concatenate：数组合并，axis=1 横向合并，注意rf_oof_train是经过reshape（-1,1）
x_train = np.concatenate((rf_oof_train, ada_oof_train, et_oof_train, gb_oof_train, dt_oof_train, knn_oof_train, svm_oof_train), axis=1) 
x_test = np.concatenate((rf_oof_test, ada_oof_test, et_oof_test, gb_oof_test, dt_oof_test, knn_oof_test, svm_oof_test), axis=1)




